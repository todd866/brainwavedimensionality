\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{booktabs}

\title{The Dimensional Hierarchy of Cortical Oscillations:\\
From Analog Substrate to Symbolic Codes}

\author{
Ian Todd\\
\textit{Sydney Medical School, University of Sydney}\\
\texttt{itod2305@uni.sydney.edu.au}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Influential work suggests that low-frequency brain oscillations implement ``low-dimensional'' control, while gamma activity supports ``high-dimensional'' information processing. We argue this framing commits a category error by conflating three distinct notions: \emph{substrate dimensionality} (how many oscillators participate coherently), \emph{interface dimensionality} (how many degrees of freedom a readout exposes), and \emph{expressed structure} (the complexity of downstream patterns). Using graph Laplacian analysis on modular networks, we show that slow eigenmodes engage substantially more oscillators than fast modes ($r = -0.75$), establishing slow waves as high-participation \emph{substrates}---even though they appear smooth through low-dimensional measurement \emph{interfaces}. Using encoder-decoder networks, we demonstrate that different interface widths support qualitatively different computations: $k=2$ produces discrete categorical codes; $k \geq 3$ preserves continuous dynamics capable of representing periodic processes without cycle aliasing (where distinct iterations become observationally identical); low-rank readout ($k \approx 1$--3) functions as mode selection rather than representation. We propose that emotion operates as an endogenous low-rank interface: a dimensionally constrained readout that ``diffracts'' through coupling matrices to produce system-wide effects---explaining why strong emotion feels like the whole brain agrees. The capacity to sustain high-rank interfaces ($k \geq 3$) despite ongoing low-rank affective readouts may be a geometric signature of cognitive regulatory capacity. Noise degrades interface rank: stress-induced collapse to $k=2$ or lower is not irrationality but an adaptive response when channel capacity is compromised.
\end{abstract}

\vspace{0.5em}
\noindent\fbox{\parbox{\dimexpr\textwidth-2\fboxsep-2\fboxrule}{%
\textbf{Terminology note.} Throughout this paper, ``substrate dimensionality'' refers to \emph{spatial participation}---how many oscillators contribute to a mode (measured by participation ratio, PR)---\textbf{not} the intrinsic dimensionality of state-space trajectories (as measured by PCA, factor analysis, or manifold learning). These are distinct quantities that can vary independently: a slow wave may engage many oscillators (high PR) while tracing a simple trajectory (low latent dimension), whereas local gamma bursts may engage few oscillators (low PR) while exhibiting complex dynamics (high latent dimension).
}}
\vspace{0.5em}

\noindent\textbf{Keywords:} cortical oscillations, information bottleneck, participation ratio, graph Laplacian, dimensional hierarchy, neural coding

\section{Introduction}

\subsection{The Dimensionality Confusion}

Influential work on oscillatory dynamics in prefrontal cortex suggests that different frequency bands serve distinct computational roles \citep{miller2018working,lundqvist2016gamma}. Low-frequency oscillations are often characterised as ``low-dimensional'' coordinating signals, while gamma activity is associated with ``high-dimensional'' information processing \citep{bastos2015visual}. Meanwhile, large-scale neural recordings reveal that population activity can occupy surprisingly high-dimensional spaces \citep{cunningham2014dimensionality,stringer2019spontaneous}.

We argue that this apparent contradiction reflects a category error: the field conflates three distinct notions of dimensionality that can systematically come apart:

\begin{enumerate}
    \item \textbf{Substrate dimensionality}: How many degrees of freedom participate coherently in the underlying neural state---the ``canvas'' on which computation occurs.
    \item \textbf{Interface dimensionality}: How many degrees of freedom are exposed at a readout or measurement---the ``aperture'' through which state is observed or transmitted.
    \item \textbf{Expressed structure}: The complexity of patterns that emerge downstream of an interface---which can be \emph{richer} than the interface rank would suggest.
\end{enumerate}

\noindent Formally, let the substrate state be $\mathbf{x}(t) \in \mathbb{R}^N$, where $N$ is the number of oscillators. The interface is a projection $\mathbf{y}(t) = \mathbf{W}\mathbf{x}(t)$, where $\mathbf{W}$ is a $k \times N$ matrix with $\text{rank}(\mathbf{W}) = k \ll N$. The expressed structure is a downstream function $\mathbf{z}(t) = F(\mathbf{y}(t))$. The participation ratio (PR) is a property of substrate modes (how many elements of $\mathbf{x}$ contribute); $k$ is a property of the interface (how many dimensions survive projection). These are independent quantities: high-PR modes can project through low-$k$ interfaces, and vice versa.

\textbf{Ontological note.} At the substrate level, cortical activity is naturally modeled as coupled dissipative fields over space (membrane potentials, synaptic currents)---elements of a function space that is formally infinite-dimensional. However, empirical access is necessarily finite-rank: electrodes, imaging, and downstream readouts impose projections that yield an \emph{effective} dimensionality determined by sampling density, noise, and readout architecture. Our use of participation ratio (PR) targets spatial participation of modes within this projected representation, not the ontological dimensionality of the underlying field. We use ``infinite-dimensional'' in the dynamical-systems sense of PDE state spaces, not to claim that measured neural trajectories exhibit arbitrarily high latent dimension.

The key insight, borrowed from wave optics, is that \textbf{dimensional collapse at an interface does not destroy degrees of freedom---it forces them to reappear in conjugate coordinates}. When light passes through a narrow slit (a 1D spatial constraint), it diffracts into a rich angular pattern. The diffraction fringes are not created by the slit; they are \emph{revealed} by it, as hidden phase relationships become legible. Similarly, a low-dimensional neural readout can produce system-wide structure by recruiting many downstream processes---what we call ``cognitive diffraction.''

This reframing resolves the apparent paradox. A slow wave sweeping across cortex may appear ``simple'' at a single electrode (low interface dimensionality) but coordinates thousands of oscillators into coherent phase relationships (high substrate dimensionality). The smoothness is not evidence of simplicity; it is the expected signature of concentration of measure in high-dimensional systems. Conversely, a gamma burst may exhibit complex temporal structure but engage only a small cortical population---genuinely low substrate dimensionality, despite high-frequency content.

\subsection{The Hierarchy Hypothesis}

We propose that the frequency spectrum implements a \emph{dimensional hierarchy}---a cascade of information bottlenecks characterised by spatial participation (how many oscillators engage coherently), not latent state-space dimensionality. As a working hypothesis, we suggest the following mapping between bands and effective bottleneck widths:

\begin{center}
\textbf{Table: Working hypothesis---frequency bands as bottleneck regimes}\\[0.5em]
\begin{tabular}{lccc}
\toprule
\textbf{Band} & \textbf{Bottleneck} & \textbf{Topology} & \textbf{Function} \\
\midrule
Delta/Theta & $k \gg 3$ & Volumetric & Raw substrate \\
Beta & $k \approx 3$ & Compliant manifold & Manipulation, meta-cognition \\
Gamma & $k \approx 2$ & Discrete clusters & Symbols, decisions \\
\bottomrule
\end{tabular}
\end{center}

The key insight is that \textbf{different bottleneck widths support qualitatively different computations}. At $k=2$, the system is forced to discretise---continuous manifolds collapse into distinct attractor basins (``symbols''). At $k \geq 3$, the system retains enough dimensionality to represent continuous processes, including self-referential structures that would suffer \emph{cycle aliasing} in lower dimensions---where distinct timepoints in a recurring process become observationally indistinguishable.

\subsection{Hypothesis: Regulatory Capacity as Dimensional Capacity}

We further hypothesise that the capacity to sustain $k \geq 3$ dynamics---to hold contradictions, ambiguity, and nuance without forcing premature resolution---constitutes a geometric signature of cognitive and emotional \emph{regulatory capacity}. Under this hypothesis, a system with reduced regulatory capacity (whether due to stress, fatigue, or developmental stage) collapses to $k=2$: black/white, good/bad, us/them. A system with high regulatory capacity can inhabit the ``compliant'' space where ambiguity coexists with ongoing processing.

Long-wavelength (slow) oscillations may provide the \emph{temporal stability} required for cycle-aliasing-free representations. When slow-wave power is reduced (stress, sleep deprivation, early developmental stage), the system may lose the substrate needed to maintain $k \geq 3$ dynamics and default to rigid categorical processing. We use ``regulatory capacity'' rather than ``maturity'' to emphasise that this is a state-dependent property, not solely a developmental one.

\section{Methods}

\subsection{Laplacian Eigenmodes and Participation Ratio}

Graph Laplacian eigenmodes provide a natural basis for analysing spatially extended oscillatory patterns \citep{atasoy2016connectome}. Lower eigenvalues correspond to smoother, longer-wavelength \emph{spatial} modes; higher eigenvalues correspond to more localised, shorter-wavelength spatial modes. Under diffusive dynamics on the graph, eigenvalue $\lambda$ relates to characteristic timescale as $\tau \propto 1/\lambda$. We use normalised eigenvalue as a \emph{spatial smoothness index}: higher $\lambda$ corresponds to more spatially localised modes (shorter wavelength), not directly to physiological oscillation frequency. The mapping from Laplacian eigenvalue to temporal frequency bands (delta, theta, gamma, etc.) is indirect and approximate; our claim concerns the \emph{geometric availability} of spatial patterns, not a one-to-one eigenvalue-to-Hz correspondence.

\textbf{Relationship to physiological oscillations.} We analyse these \emph{structural} eigenmodes as defining the geometric constraints---the ``manifold capacity''---within which neural dynamics unfold. Laplacian modes are most directly relevant when coupling is approximately diffusive (gap junctions, ephaptic fields, or weak synaptic coupling in the linearised regime). Real neural dynamics involve delays, inhibition, and nonlinearities that can deviate from this basis. However, the Laplacian spectrum defines what spatial patterns are \emph{available} to the network: slow structural modes are geometrically capable of recruiting many oscillators; fast modes are not. Whether physiological rhythms exploit this capacity depends on the dynamics, but the capacity itself is a structural property. Our claim is thus about \emph{geometric substrate availability}, not a direct mapping from eigenvalue to EEG frequency band.

\subsubsection{Network Construction}

We model cortical connectivity using a modular (stochastic block) network with $N = 2500$ nodes organised into 25 modules of 100 nodes each. Connection probability within modules was $p_{\text{within}} = 0.3$; between modules, $p_{\text{between}} = 0.01$.

The unnormalised graph Laplacian $L$ has diagonal entries equal to node degree and off-diagonal entries of $-1$ for each edge:
\begin{equation}
    L_{ij} = \begin{cases}
        \deg(i) & \text{if } i = j \\
        -1 & \text{if } i \sim j \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

\subsubsection{Participation Ratio}

The participation ratio (PR), originally developed to quantify localisation in disordered solids \citep{bell1970atomic}, measures how many oscillators contribute significantly to a given mode. For a normalised eigenmode $\psi$ across $N$ oscillators:
\begin{equation}
    \text{PR}(\psi) = \frac{1}{\sum_i |\psi_i|^4}, \quad \text{with } \sum_i |\psi_i|^2 = 1.
\end{equation}

PR $\approx N$ when all oscillators contribute equally; PR $\approx 1$ when activity is confined to a single oscillator. We computed the 150 smallest eigenpairs using ARPACK. The negative correlation between eigenvalue and PR is strongest for modular networks ($r \approx -0.85$) and weaker for simple lattices ($r \approx -0.22$); small-world networks (Watts-Strogatz with random rewiring but no modular structure) show a reversed pattern ($r \approx +0.73$; see Appendix, Figure~\ref{fig:robustness_laplacian}). This topology dependence is itself a prediction: the slow$>$fast PR effect requires distinct functional modules with sparse inter-module connections. Cortical connectivity is both small-world \emph{and} modular \citep{sporns2016modular}---the modular structure, not the small-world property per se, drives the effect. We therefore predict that the PR hierarchy should be strongest in association cortex (pronounced columnar modularity) and weaker in early sensory areas (more homogeneous connectivity).

\textbf{Note on dimensionality terminology.} PR quantifies \emph{spatial participation}---how many oscillators contribute to a given mode---not the intrinsic dimensionality of the state-space manifold (as typically measured by PCA, factor analysis, or manifold learning). When we say ``high-dimensional'' in this paper, we mean ``many units participate coherently'' unless otherwise specified. Our claim is about the geometric substrate: slower modes engage more degrees of freedom, providing a higher-capacity ``canvas.''

\subsubsection{Synthetic Time-Series Demonstration}

To demonstrate that the participation ratio metric correctly distinguishes global from local activity in time-series data, we generated synthetic multi-channel signals with known spatial structure. Slow activity (2--8 Hz) was simulated as a global oscillation with high inter-channel correlation ($r = 0.8$); fast activity (30--50 Hz) was simulated as sparse, uncorrelated bursts. The combined signal was bandpass filtered and PR was computed on amplitude envelopes over 200 ms sliding windows. This is a \emph{sanity check}, not a validation: the synthetic signals are constructed to have the property we then recover. The demonstration confirms that PR behaves as expected when spatial structure is uncontaminated by volume conduction, but does not test the metric's robustness to realistic mixing or noise.

\subsection{Bottleneck Compression}

The information bottleneck principle \citep{tishby2000information} formalises the trade-off between compression and preservation of task-relevant information. We use encoder-decoder networks to explore how bottleneck dimensionality affects code formation.

\subsubsection{Category Construction}

We defined six categories as smooth phase gradient patterns across 256 oscillators:
\begin{equation}
    \phi_c(i) = 0.3\cos(x_i + \theta_c) + 0.2\sin(0.5x_i + \theta_c)
\end{equation}
where $x_i \in [0, 4\pi]$ and $\theta_c = \pi c / 6$. Samples were generated by adding Gaussian noise ($\sigma = 0.5$), yielding 200 samples per category. We chose phase-gradient inputs not to force circular topology arbitrarily, but because grid cells, head direction cells, and theta phase precession demonstrate that the brain fundamentally encodes continuous variables using cyclic phase codes ($S^1$ topology).

\subsubsection{Network Architecture}

The encoder-decoder compresses 256-D input through bottlenecks of width $k \in \{1, 2, 3, 4, 8, 16, 32\}$:
\begin{itemize}
    \item \textbf{Encoder}: $256 \rightarrow 128 \rightarrow 128 \rightarrow k$ (ReLU)
    \item \textbf{Bottleneck}: $k$ dimensions with additive Gaussian noise ($\sigma = 0.5$)
    \item \textbf{Decoder}: $k \rightarrow 128 \rightarrow 128 \rightarrow 256$ (ReLU)
\end{itemize}

Training used Adam (lr = $10^{-3}$) for 150 epochs, minimising MSE reconstruction error. Code formation was measured by Adjusted Rand Index (ARI) between $k$-means clustering of bottleneck codes and true labels.

\subsection{Cycle Aliasing in Periodic Processes}

To demonstrate cycle aliasing, we simulated a periodic re-entrant process---a system that repeatedly cycles through similar states while accumulating context (e.g., working memory maintenance, predictive coding loops, or iterative evaluation). The Liar's Paradox (TRUE $\rightarrow$ FALSE $\rightarrow$ TRUE $\rightarrow$ ...) provides an intuitive example, but the geometric point applies to any periodic process with hidden cycle index. We embedded the trajectory in 2D versus 3D spaces:

\begin{itemize}
    \item \textbf{2D embedding}: The cyclic trajectory traces a closed loop; under linear readout, distinct phases of successive cycles map to the same 2D location (cycle aliasing).
    \item \textbf{3D embedding}: The trajectory spirals upward (helix), with time/context as the third dimension, separating successive cycles.
\end{itemize}

Self-intersections were counted as pairs of trajectory points that are spatially close ($<0.15$ units) but temporally distant ($>10$ steps apart). We also trained a linear autoencoder to compress and reconstruct the 3D helix through $k=2$ versus $k=3$ bottlenecks, quantifying the reconstruction error under linear (PCA-like) interface constraints.

\section{Results}

We present three linked results that together demonstrate the dimensional hierarchy. First, we show that slow modes are geometrically high-dimensional---they engage many oscillators coherently (Section~3.1). Second, we show that a narrow bottleneck acting on this substrate naturally produces discrete codes at $k \approx 2$--3 (Section~3.2). Third, we demonstrate that $k \geq 3$ is required to represent self-referential dynamics without cycle aliasing under linear interface constraints (Section~3.3). Finally, we show that increased noise shifts the optimal bottleneck width downward, providing a mechanism for stress-induced categorical collapse (Section~3.4).

\subsection{Slow Modes Have Higher Participation}

Figure~\ref{fig:laplacian} shows participation ratio versus normalised eigenvalue. There is a strong negative correlation ($r = -0.75$, $p < 0.001$): slower modes engage substantially more oscillators. The slowest 15 modes have mean PR $\approx$ 560 (23\% of nodes); the fastest 15 modes have mean PR $\approx$ 175 (7\% of nodes)---a 3-fold difference. This effect is largest in modular networks; lattice graphs show weaker correlations and small-world graphs can show reversed patterns (Appendix, Figure~\ref{fig:robustness_laplacian}), predicting that the slow$>$fast PR difference should be strongest in association cortex with pronounced modularity. \textbf{Limitation}: These results are from a single network realisation; future work should verify robustness across an ensemble of random network draws and modularity parameter sweeps.

The synthetic time-series demonstration (Figure~\ref{fig:synthetic}) confirms that PR behaves as expected, distinguishing global from local activity. Slow-band activity (2--8 Hz) engaged 99\% of channels (PR = 63.2/64); fast-band activity (30--50 Hz) engaged only 42\% (PR = 26.6/64)---a 2.4-fold difference matching the Laplacian prediction. (Preliminary application to 64-channel scalp EEG yielded null results, consistent with the spatial-blurring limitations discussed in Section~\ref{sec:limitations}.)

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/fig1_laplacian_participation.pdf}
    \caption{\textbf{Long-wavelength modes engage more oscillators.} (A) Participation ratio decreases with normalised Laplacian eigenvalue (spatial smoothness index; $r = -0.75$). Lower eigenvalue = spatially smoother mode. (B) Mean participation for smoothest (``slow'') vs most localised (``fast'') modes shows 3-fold difference. (C) Activation distributions: spatially smooth modes spread activity broadly across the network.}
    \label{fig:laplacian}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig2_synthetic_pr_validation.pdf}
    \caption{\textbf{Synthetic demonstration of PR metric behaviour.} (A) PR distributions for slow (2--8 Hz) versus fast (30--50 Hz) bands in synthetic multi-channel data with known spatial structure. (B) Mean PR: slow band engages 99\% of channels; fast band engages 42\%. This 2.4-fold difference is a sanity check confirming that PR behaves as expected when spatial structure is preserved. It does not validate the metric against realistic mixing or noise.}
    \label{fig:synthetic}
\end{figure}

\subsection{Discrete Codes Emerge at $k=2$}

Figure~\ref{fig:bottleneck} shows that code formation peaks at the critical bottleneck width $k \approx 2$--3 (ARI = 0.88; representative run with fixed seed). At $k=1$, information is lost (ARI = 0.64). At $k \geq 4$, discretisation pressure diminishes and codes become more distributed (ARI = 0.82--0.87). \textbf{Limitation}: These results are from a single random seed; future work should report variability across seeds and compare against a linear (PCA-based) baseline to isolate the contribution of nonlinear encoding.

The critical $k \approx 2$ reflects a \emph{static} topological constraint: the input categories are distinguished by phase relationships with $S^1$ (circular) topology. In our construction, they are embedded as a circle in $\mathbb{R}^2$, so a 1-D bottleneck cannot separate them, whereas $k \geq 2$ is sufficient for static classification. However, as Section~\ref{sec:paradox} demonstrates, when \emph{dynamical} trajectories must evolve over time without cycle aliasing, an additional dimension ($k \geq 3$) becomes functionally important under linear readout---the distinction between ``enough dimensions to separate categories'' and ``enough dimensions to preserve process-state distinctness across cycles.''

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/fig3_code_formation_bottleneck.pdf}
    \caption{\textbf{Discrete codes emerge at critical capacity.} (A) Code formation (ARI) peaks at $k \approx 2$--3, reflecting the $S^1$ topology of the phase-defined categories. (B) Bottleneck codes at $k=2$ form six distinct clusters, each interpretable as a proto-symbol. (C) Schematic: high-dimensional slow-wave patterns compress through a noisy bottleneck, forcing discretisation when channel capacity is limited.}
    \label{fig:bottleneck}
\end{figure}

\subsection{Periodic Processes Require $k \geq 3$ to Avoid Cycle Aliasing}
\label{sec:paradox}

Figure~\ref{fig:paradox} demonstrates both geometric and computational evidence for the $k \geq 3$ requirement under linear interface constraints. A periodic re-entrant trajectory (illustrated with the Liar's Paradox as an intuitive example) produces 1511 self-intersections when confined to 2D (panel A); in 3D, it forms a helix that separates successive cycles (panel B). The computational illustration is equally clear: a linear autoencoder trained to compress and reconstruct the 3D helix through a $k=2$ bottleneck fails on the time/context dimension (MSE = 0.108), while $k=3$ achieves near-perfect reconstruction (MSE $< 10^{-6}$; panels C--D).

The key insight is \emph{cycle aliasing}: under a linear (PCA-like) interface that discards the context dimension, distinct phases of successive cycles become observationally identical. The system cannot distinguish ``first pass through state A'' from ``second pass through state A'' because both map to the same 2D location. A nonlinear decoder could, in principle, encode the cycle index in fewer dimensions---but this would require explicitly representing a monotonic context variable. Our claim is that \emph{linear} readouts (which approximate many biological interfaces) require $k \geq 3$ to preserve process-state distinctness across cycles. The Liar's Paradox is merely an illustrative example; the geometric constraint applies to any periodic process with hidden cycle index (working memory refresh, predictive coding, re-entrant evaluation).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/fig4_paradox_topology.pdf}
    \caption{\textbf{Periodic processes require $k \geq 3$ to avoid cycle aliasing.} (A) In 2D, a periodic re-entrant trajectory (Liar's Paradox as illustrative example) suffers cycle aliasing (1511 intersections; point pairs $<0.15$ units apart but $>10$ steps distant). Colour encodes time/context. (B) In 3D, the trajectory forms a helix that separates successive cycles. (C) Learning dynamics: $k=2$ plateaus at high MSE; $k=3$ converges to near-zero. (D) Context reconstruction: $k=2$ cannot recover the cycle index because aliasing forces distinct contexts to the same bottleneck state (red); $k=3$ preserves process-state distinctness (green). The geometric point generalises beyond paradoxes to any periodic process with hidden cycle index.}
    \label{fig:paradox}
\end{figure}

\subsection{Noise Shifts the Optimal Bottleneck}

Figure~\ref{fig:stress} shows that the optimal bottleneck width depends on channel noise. Quantitatively, the argmax over $k$ shifts from $k \geq 3$ at $\sigma \leq 0.3$ to $k = 2$ at $\sigma \geq 0.7$. High-dimensional ``helical'' representations are fragile---they require precise coordination across many dimensions. Binary contrast ($k = 2$) is robust: it survives noise because it only needs to distinguish two categories. This provides a computational mechanism for the stress--rigidity link: degraded signal-to-noise ratio (from stress, fatigue, or developmental immaturity) drives the system toward categorical processing not by choice but by information-theoretic necessity. The peak at $k \approx 2$--3 generalises across different numbers of input categories (Appendix, Figure~\ref{fig:robustness_categories}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig5_stress_collapse.pdf}
    \caption{\textbf{Noise forces topological collapse.} Code formation (ARI) heatmap across bottleneck width ($k$) and channel noise ($\sigma$). At low noise (bottom), the system supports higher-dimensional nuance ($k \geq 3$). At high noise (top), optimal coding collapses to $k = 2$: binary categorisation becomes the only robust strategy. This provides a computational mechanism for stress-induced rigid thinking.}
    \label{fig:stress}
\end{figure}

\section{Low-Rank Interfaces and Global Recruitment}

The bottleneck experiments (Section~3.2) demonstrate that $k=2$ produces discrete categorical codes while $k \geq 3$ preserves continuous dynamics. But there is a third regime below $k=2$ that merits separate treatment: the \emph{rank-1 interface}, where readout is restricted to a single dimension.

\subsection{Affective Readout as Low-Rank Projection}

Consider a high-dimensional brain state $\mathbf{x} \in \mathbb{R}^N$ being read out through a low-rank projection:
\begin{equation}
    \mathbf{e} = \mathbf{U}^\top \mathbf{x} \in \mathbb{R}^m, \quad m \in \{1, 2, 3\}
    \label{eq:lowrank}
\end{equation}
where $\mathbf{U}$ is an $N \times m$ matrix defining the readout subspace. This is a low-rank interface---a narrow ``slit'' through which the high-dimensional state is observed. In the limiting case of $m=1$, a single scalar is extracted from the entire $N$-dimensional state.

We propose that \textbf{emotion functions as an endogenous low-rank readout channel}, typically operating in the range $m \approx 1$--3. The affective system monitors global state (arousal, valence, threat level) without representing its full high-dimensional structure \citep{barrett2017emotions,damasio1994descartes}. Under normal conditions, affective readout may be rank-2 or rank-3, preserving some distinctions (valence $\times$ arousal, or approach/avoid $\times$ certainty). Under urgency or stress, affective readout may collapse toward rank-1: a single dominant axis (safe/dangerous, good/bad) that overwhelms finer distinctions.

Equation~\ref{eq:lowrank} is a mathematical idealisation of how interoceptive and limbic circuits compress distributed cortical states into low-dimensional affective signals \citep{craig2009interoception,kleckner2017arousal}. The body's readout of the brain is dimensionally constrained---you feel a \emph{few} integrated dimensions of state (alertness, valence, motivation), not 10,000 separate measurements.

\subsection{Diffraction: Low-Rank Readout, High-Rank Effect}

What happens after a low-rank affective readout? The signal does not simply pass to motor output; it modulates the entire system:
\begin{equation}
    \mathbf{x}' = \mathbf{x} + g(\mathbf{e}) \cdot \mathbf{A}\mathbf{x}
    \label{eq:diffraction}
\end{equation}
where $g(\mathbf{e})$ is a nonlinear gain function of the low-rank affective signal and $\mathbf{A}$ is a coupling matrix that broadly recruits attention, memory, motor planning, and interpretive circuits.

This is \emph{cognitive diffraction}: the low-rank signal ``diffracts'' through the coupling structure to produce system-wide effects. Just as light through a narrow slit produces rich diffraction fringes, the low-dimensional emotional readout produces complex, coordinated downstream patterns.

This explains why strong emotion \emph{feels} like the whole brain agrees. A low-rank readout (approaching rank-1 under urgency: ``this is threatening'') recruits a coherent policy across subsystems: attention narrows, memory retrieval biases toward threat-relevant episodes, motor planning shifts toward fight-or-flight, interpretation of ambiguous stimuli shifts toward hostile. The richness is real---but it emerges \emph{downstream} of the slit, not upstream.

\subsection{The Tri-Level Regime}

We can now distinguish three interface types with qualitatively different computational properties:

\begin{center}
\begin{tabular}{lcll}
\toprule
\textbf{Interface Type} & \textbf{Rank} & \textbf{Function} & \textbf{Example} \\
\midrule
Affective readout & $k \approx 1$--3 (low-rank) & Mode selection, policy broadcast & Fear, anger, arousal \\
Categorical bottleneck & $k = 2$ & Discrete symbols & Decisions, labels, assertions \\
Compliant manifold & $k \geq 3$ & Analog manipulation & Reasoning, meta-cognition \\
\bottomrule
\end{tabular}
\end{center}

The key insight is that \textbf{low-rank affective interfaces are not simplified versions of categorical interfaces}. They serve a different function: not representing content but \emph{selecting the manifold on which content is processed}. Emotion picks which attractor landscape the system inhabits; schemas determine where on that landscape the trajectory can go. Under stress or urgency, affective readout collapses toward rank-1, producing the ``whole brain agrees'' phenomenology of strong emotion.

\subsection{Noise Reduces Interface Rank}

The stress simulation (Figure~\ref{fig:stress}) showed that noise shifts the optimal bottleneck from $k \geq 3$ toward $k = 2$. We now extend this logic: under extreme noise or resource constraint, even $k = 2$ may be unstable, and the system collapses further toward rank-1 dominance.

This provides a mechanistic account of affect domination under stress. The stressed or fatigued brain cannot maintain the channel capacity for nuanced schemas ($k \geq 3$) or even categorical distinctions ($k = 2$). It defaults to a low-rank readout approaching a single dominant axis: good/bad, safe/dangerous, fight/flee. This is not irrationality---it is the adaptive response when information-theoretic constraints make higher-rank interfaces unreliable.

\subsection{Regulatory Capacity as Rank Maintenance}

We can now sharpen the hypothesis from Section~1.3. Cognitive and emotional \emph{regulatory capacity} is the capacity to \textbf{sustain high-rank interfaces despite low-rank affective readouts}.

A system with high regulatory capacity experiences emotion (low-rank readout) without having behaviour dominated by it. It can register ``threat detected'' while maintaining the $k \geq 3$ dynamics needed to reason about the threat, consider its sources, and modulate response. A system with low regulatory capacity collapses: the low-rank emotional signal directly governs behaviour, bypassing the schema layer entirely.

This is not about suppressing emotion but about \emph{maintaining interface rank under affective pressure}. The slow-wave substrate (high PR, high participation) provides the stability reservoir. When that reservoir is depleted (sleep deprivation, chronic stress, early developmental stage), the system cannot buffer the low-rank signal, and cognition collapses toward affective mode. We use ``regulatory capacity'' rather than ``maturity'' to avoid normative connotations; the capacity varies with state (fatigue, stress) as well as development.

\section{Discussion}

\subsection{The Frequency Hierarchy}

These results suggest that cortical frequency bands implement a cascade of dimensional bottlenecks:

\begin{itemize}
    \item \textbf{Slow oscillations (delta, theta)}: Maintain the high-dimensional volumetric substrate. Many oscillators participate coherently, providing the ``canvas'' for computation.
    \item \textbf{Beta oscillations ($k \approx 3$)}: Intermediate compression. Beta rhythms are associated with maintenance of the current cognitive state \citep{engel2010beta}. The manifold is constrained but remains ``compliant''---continuous enough for analog manipulation, mental rotation, holding ambiguity.
    \item \textbf{Gamma oscillations ($k \approx 2$)}: Tight compression. Gamma rhythms support selective information transmission through coherence \citep{fries2015rhythms}. Forces categorical commitment---symbols, decisions, assertions. Optimal for transmission but sacrifices nuance.
\end{itemize}

The brain may dynamically adjust bottleneck width via frequency shifts: increasing beta power to ``loosen'' the constraint when flexibility is needed; increasing gamma power to ``tighten'' when commitment is required.

Recent empirical work provides direct support for this hierarchy. \citet{chen2026spatial} recorded from macaque prefrontal cortex during cognitive tasks and found that alpha/beta oscillations encode task context (rules, categories) while spiking activity carries sensory content (stimulus identity). Critically, alpha/beta power formed spatially structured ``inhibitory stencils'' across the cortical surface that gated \emph{where} sensory information could be expressed in spiking---exactly the interface/substrate distinction proposed here. Their ``spatial computing'' framework, in which cortical space serves as a computational dimension controlled by oscillatory patterns, provides empirical validation of our theoretical architecture.

\subsection{The Illusion of Low Dimensionality: Measuring the Interface, Not the Substrate}

A central premise of the ``low-dimensional subspace'' hypothesis \citep{miller2018working,lundqvist2016gamma} is that the smoothness and low-frequency nature of macroscopic brain waves implies a reduction in the degrees of freedom of the underlying neural substrate.

We argue that this inference commits the category error identified in Section~1.1: \textbf{Miller and colleagues measure interface dimensionality and mistake it for substrate dimensionality}. Their electrodes act as low-dimensional projections of a high-dimensional state. The smooth, coherent signals they observe are precisely what concentration of measure predicts when a high-dimensional system is read out through a limited channel. The regularity is a property of the measurement, not the measured.

In high-dimensional probability theory, the \emph{concentration of measure} phenomenon dictates that as the effective dimensionality of a system increases, the probability mass concentrates rapidly around specific regions---for instance, the equator of a hypersphere \citep{gorban2018blessing}. The relevant quantity is not the raw number of neurons ($N \approx 10^{11}$) but the \emph{effective} number of independently contributing units, which we approximate by the participation ratio (PR). Consider a mode with PR $= N_{\text{eff}}$: the state effectively lives in $\mathbb{R}^{N_{\text{eff}}}$, not $\mathbb{R}^N$. If we assume neural activity is bounded (e.g., on a hypersphere $S^{N_{\text{eff}}-1}$), L\'{e}vy's lemma states that for any Lipschitz continuous function $f$ (such as a local field potential measurement), the probability that $f(\mathbf{x})$ deviates from its median $M_f$ by more than $\epsilon$ decays exponentially with $N_{\text{eff}}$:
\begin{equation}
    P(|f(\mathbf{x}) - M_f| \geq \epsilon) \leq 2 \exp(-C \cdot N_{\text{eff}} \cdot \epsilon^2)
    \label{eq:levy}
\end{equation}
where $C$ is a constant depending on the Lipschitz constant of $f$. Crucially, our Laplacian analysis (Section~3.1) demonstrates that slow oscillations maximise $N_{\text{eff}}$ (high PR), whereas fast oscillations restrict it. Therefore, slow waves are uniquely subject to the stabilising effects of concentration of measure, appearing smooth and ``low-dimensional'' precisely because they are built from the largest possible number of coordinated units. The raw neuron count is irrelevant; what matters is how many units participate coherently in a given mode.

This implies that in a truly high-dimensional substrate, macroscopic measurements will essentially \emph{always} appear smooth and deterministic---not because the underlying system is simple, but because the massive dimensionality forces the system into a statistical ``shell'' of stability. The apparent low-dimensionality is a property of the \emph{measurement projection}, not the intrinsic topology of the substrate.

This reinterpretation is supported by recent high-resolution recordings. \citet{stringer2019high} showed that visual cortex activity is not low-dimensional; it obeys a power-law eigenspectrum ($\lambda_n \propto 1/n$) that fills high-dimensional space far more than smooth manifold models predict. The ``effective dimensionality'' they observe is not a property of a reduced subspace but rather the signature of concentration: very many dimensions, all aligned toward a statistical centre.

Therefore, the ``slow waves'' observed by Miller and colleagues are not evidence of a low-dimensional controller; they are the expected statistical signature of a high-dimensional substrate undergoing synchronisation. What appears as ``low-dimensional control'' is actually \emph{high-participation alignment}. The brain does not discard dimensions to create simplicity; it aligns them, and concentration of measure makes the result look simple to any finite-dimensional readout.

In the language of Section~1.1: Miller measures the \emph{interface}---the low-dimensional output of an electrode array---and infers that the \emph{substrate} is equally simple. But the interface is a ``slit.'' Its smoothness tells us about the measurement geometry, not the source geometry. The paradox dissolves once we recognise that slow waves can be simultaneously high-dimensional substrates (many oscillators coherently engaged) and low-dimensional interfaces (smooth projections easily measured).\footnote{This substrate/interface distinction has analogues in theoretical physics (e.g., holographic duality), but we rely only on the wave-optics intuition developed in Section~1.1.}

Notably, recent work from the same laboratory \citep{chen2026spatial} provides empirical support for precisely this reframing. Chen et al.\ show that alpha/beta oscillations form low-dimensional spatial patterns across PFC---but these patterns do not \emph{reduce} substrate dimensionality; they \emph{gate} it. Spiking activity (the high-dimensional content) is expressed only where alpha/beta power is low. The low-dimensional oscillatory signal acts as an interface that controls access to the high-dimensional substrate, not as a replacement for it. This ``spatial computing'' framework vindicates the distinction we draw: the same group's data reveal that ``low-dimensional control'' and ``high-dimensional substrate'' coexist, with the former regulating where the latter can be expressed.

\subsection{Cycle Aliasing and Meta-Cognition}

The cycle aliasing simulation reveals a fundamental distinction:
\begin{itemize}
    \item \textbf{$k=2$ (Gamma)}: Periodic re-entrant processes produce cycle aliasing. Under a linear readout, the system cannot distinguish which iteration of a recurring process it occupies---successive passes through similar states map to the same low-dimensional representation. This ambiguity can produce oscillation, confusion, or forced premature resolution.
    \item \textbf{$k \geq 3$ (Beta)}: The extra dimension separates successive cycles. The system can represent the \emph{process}---track where it is in an ongoing evaluation---without collapsing distinct iterations.
\end{itemize}

This offers a geometric interpretation of meta-cognitive capacity: the ability to hold a process ``in mind'' while it unfolds, rather than being forced to immediate resolution. The Liar's Paradox is an extreme case where resolution is impossible; more mundane examples include working memory maintenance (distinguishing current from previous items), predictive coding (distinguishing prediction from observation), and deliberation (distinguishing tentative from committed states). Other encoding schemes (e.g., discrete state machines with explicit cycle counters) can represent these distinctions without an extra spatial dimension; our point is that \emph{continuous geometric} representations under linear interface constraints naturally require $k \geq 3$ to preserve process-state distinctness.

\subsection{Hypothesis: Regulatory Capacity and Long-Wavelength Stability}

We hypothesise that the capacity to sustain $k \geq 3$ dynamics constitutes a geometric signature of cognitive and emotional regulatory capacity:

\begin{itemize}
    \item \textbf{Reduced regulatory capacity} (stress, fatigue, early development): Tends toward an effectively $k \approx 2$ regime, favouring binary categorisation. Contradictions may be difficult to tolerate; ambiguity may provoke discomfort or forced resolution.
    \item \textbf{High regulatory capacity}: Can sustain $k \geq 3$ dynamics. Holds contradictions without forcing resolution. Tolerates ambiguity. Maintains multiple perspectives simultaneously.
\end{itemize}

This geometric capacity likely tracks both state (fatigue, stress) and development. The prefrontal cortex (PFC) is the primary generator of top-down beta oscillations \citep{miller2018working,lara2015prefrontal} and is the last cortical region to fully myelinate, a process that continues into the mid-20s \citep{gogtay2004dynamic}. The ability to sustain the ``helical'' $k \geq 3$ dynamics required for cycle-aliasing-free processing may be physically rate-limited by this development. A developing brain, lacking the myelinated bandwidth to sustain stable high-dimensional beta loops, may fall back on earlier-developing, sensory-driven gamma circuitry ($k=2$). Consequently, it may default to a ``planar'' topology where ambiguity forces immediate, binary resolution rather than being held in suspension.

Long-wavelength oscillations provide the \emph{temporal stability} needed for this capacity. Slow waves have long autocorrelation times---they change gradually, providing a stable backdrop against which complex, cycle-separated trajectories can unfold. Slow-wave sleep, in particular, is critical for memory consolidation \citep{diekelmann2010memory}; its disruption impairs not only declarative memory but also the cognitive flexibility that depends on integrated representations. When slow-wave power is compromised (stress, sleep deprivation, early developmental stage), the system loses this stability and collapses to categorical processing. The noise simulation (Figure~\ref{fig:stress}) formalises this intuition: as channel signal-to-noise ratio degrades, the optimal bottleneck width shifts from $k \geq 3$ (nuance) to $k = 2$ (binary contrast). This is not a failure of the system but an adaptive response---binary codes are simply more robust to noise. While we model this mathematically as additive noise, biologically it corresponds to any reduction in effective channel capacity: synaptic fatigue, inhibitory dysregulation, or information overload where input complexity exceeds processing bandwidth. These developmental and clinical links are speculative hypotheses that would require careful, dedicated empirical studies to test.

\subsection{Cross-Frequency Coupling}

The well-documented coupling between slow-wave phase and gamma amplitude \citep{canolty2010functional,lisman2013theta} may implement exactly this hierarchy. The slow oscillation defines the current position on the high-dimensional substrate; the gamma burst transmits a discrete ``snapshot'' of that position. The number of distinct gamma codes per slow cycle would be limited by $k=2$ capacity, potentially explaining the 7$\pm$2 limit on working memory items \citep{miller1956magical,cowan2001magical,lisman2013theta}.

\subsection{Laminar and Bioelectric Connections}

This architecture maps onto cortical laminar structure: deep layers (L5/6) with extensive horizontal connectivity support the slow, high-dimensional substrate; superficial layers (L2/3) with gamma-dominant activity implement the bottleneck \citep{vankerkoerle2014alpha}. The laminar segregation of alpha/beta (deep, feedback) versus gamma (superficial, feedforward) aligns precisely with the proposed hierarchy.

More broadly, the motif of electric fields interacting with 2D surfaces recurs across biology: cell membranes, bioelectric gradients in morphogenesis \citep{levin2021bioelectric}, and cortical sheets all share this geometric configuration. Ephaptic coupling---where neurons influence each other through extracellular fields rather than synapses \citep{anastassiou2011ephaptic}---represents the same principle operating at faster timescales; Levin's bioelectric code and neural ephaptic fields may be two expressions of a common computational motif. The 2D surface may be a broadly useful architecture for transforming continuous volumetric dynamics into discrete signals.

\subsection{Testable Predictions}

The framework generates several concrete predictions for future empirical work:

\begin{enumerate}
    \item \textbf{PR across frequency bands}: In dense intracranial recordings (Utah arrays, high-density ECoG), beta-band activity should engage more channels (higher PR) than gamma-band activity during flexible cognition tasks, while both should engage more than rest.
    \item \textbf{State-space trajectories}: During tasks requiring cognitive flexibility (e.g., set-shifting), decoded neural trajectories should exhibit more continuous, less clustered structure than during categorical decision tasks.
    \item \textbf{Cross-frequency structure}: The number of distinguishable gamma ``packets'' per slow-wave cycle should be constrained to approximately $k=2$ worth of information, potentially explaining working memory capacity limits.
    \item \textbf{Developmental trajectory}: PR measures for beta-band activity in frontal regions should increase with age through adolescence, paralleling PFC myelination.
\end{enumerate}

\subsection{Limitations and Measurement Requirements}
\label{sec:limitations}

While synthetic validation confirms the PR metric's utility (Figure~\ref{fig:synthetic}), preliminary application to scalp EEG (64-channel recordings) yielded null results, as did exploratory analysis of intracranial SEEG recordings from epilepsy patients (OpenNeuro ds004100; $n=10$ subjects showed a trend in the expected direction with mean slow/fast ratio 1.14$\times$, but high inter-subject variability, likely reflecting pathological tissue and clinically-driven electrode placement). These null or noisy results are consistent with the spatial Nyquist theorem: capturing the geometric difference between global and local modes requires sampling density higher than the spatial frequency of the mode. Scalp EEG (inter-electrode distance $\sim$2--3 cm) spatially aliases local gamma structure, making it indistinguishable from noise. The slow-wave versus fast-wave PR difference requires electrode spacing finer than the spatial scale of functional oscillator units. Cortical columns span $\sim$300--500 $\mu$m; resolving participation at this scale requires Utah-array density ($\sim$400 $\mu$m pitch) or high-density ECoG. Standard ECoG grids (1 cm spacing) may still average over too many columns. This represents a clear experimental target: PR analysis on dense intracranial recordings during tasks requiring flexibility (beta-dominant) versus commitment (gamma-dominant). Additionally, PR computed on sensor-space amplitude envelopes is confounded by volume conduction and reference scheme; future tests should use bipolar/local referencing or connectivity measures less sensitive to zero-lag mixing.

The models presented here isolate geometric and information-theoretic aspects; they do not capture excitatory-inhibitory dynamics, conduction delays, or synaptic nonlinearities. Additionally, Laplacian eigenmodes represent standing wave constraints based on diffusion; while axonal delays enable traveling waves that deviate from these standing modes, the Laplacian defines the structural manifold capacity---the baseline geometric availability of the network before temporal dynamics are applied. Future work should test these predictions in biologically realistic spiking networks and empirical data with appropriate spatial resolution.

\section{Conclusion}

Cortical oscillations implement a dimensional hierarchy: slow waves maintain the high-dimensional analog substrate; beta provides intermediate ``compliant'' compression for manipulation; gamma enforces discrete symbol formation. The capacity to sustain $k \geq 3$ dynamics---to hold paradoxes without cycle aliasing---may be a geometric signature of maturity. Long-wavelength stability provides the temporal substrate for flexible cognition; its disruption forces the mind into rigid categorical processing.

Intelligence emerges from the controlled collapse of analog into digital. The canvas is slow; the brushstrokes are fast.

\section*{Acknowledgements}

The author thanks the Fulcher Lab, University of Sydney, for feedback on an early version of this work, and Prof.\ Michael Levin (Tufts University) for generously answering questions at office hours---particularly the insight that the Liar's Paradox can be understood as an oscillation in phase space.

\section*{Statements and Declarations}

\noindent\textbf{Funding.} The author did not receive support from any organisation for the submitted work.

\noindent\textbf{Competing Interests.} The author has no relevant financial or non-financial interests to disclose.

\noindent\textbf{Code Availability.} All simulation code and analysis scripts are available at: \url{https://github.com/todd866/brainwavedimensionality}

\noindent\textbf{AI Assistance.} Large language models (Claude, GPT) were used for AI-assisted copy editing and iterative manuscript refinement. The author takes full responsibility for the content.

\appendix

\section{Robustness Analyses}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/figS1_laplacian_robustness.pdf}
    \caption{\textbf{Participation ratio effect depends on modular structure.} (A) Correlation between eigenvalue and PR across network types. The negative correlation is strongest for modular networks ($r \approx -0.85$), weaker for lattices ($r \approx -0.22$), and reverses for small-world networks without modular structure ($r \approx +0.73$). (B) PR ratio (slowest 10 modes / fastest 10 modes) confirms modular networks show the largest slow/fast difference. This topology dependence generates a testable prediction: the PR hierarchy should be strongest in association cortex (pronounced columnar modularity) and weaker in sensory areas with more homogeneous connectivity.}
    \label{fig:robustness_laplacian}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/figS2_category_sweep.pdf}
    \caption{\textbf{Code formation peaks at $k \approx 2$--3 across category counts.} ARI versus bottleneck width for different numbers of input categories (3, 6, 9, 12). Regardless of category count, peak code formation occurs at $k = 2$--3, confirming this is a robust feature of the bottleneck architecture rather than an artefact of the specific 6-category construction used in the main text.}
    \label{fig:robustness_categories}
\end{figure}

\clearpage

\bibliographystyle{apalike}
\bibliography{references}

\end{document}
